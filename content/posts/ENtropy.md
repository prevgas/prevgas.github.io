+++
title = 'Entropy'
draft = false
+++
entropy는 정보의 불확실성을 나타낸다. 

그러면 정보란 무엇일까? 정보에대한 정의는 무수히 많지만 엔트로피에서 사용되는 정보란
특정사건에대한 확률을 물리량으로 나타낸것으로 볼 수 있다.

하나의 예시를 들어보자.
유한한 표본공간에 하나의 사건만 있을 경우에 이 데이터를 저장하려면 얼마나 필요한지 생각해보자 답은 필요하지 않다.
그 공간에서 확률(믿음의 정도)를 저장해야할 필요가 없다 무조건 1이기때문이다.
그러면 두 가지인 경우에는 컴퓨터에 확률을 저장한다하면 1비트가 필요한다.
1비트에 두가지 경우수를 표현할 수 있다.
그래서 정보는  logp(x) 밑이 무엇에따라 정보단위가 바뀐다.
기본적으로 2로하면 단위가 비트이다. 넘어가서 엔트로피 정의는 정보의 불확실성을 의미한다. 수식은 $\sum_{i=1}^{n}p(x)logp(x)$ 이다.


